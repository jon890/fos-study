# Spring Batch Study

## 용어 정리

### Job

- 하나의 완전한 배치 처리, 우리가 흔히 접하는 배치 작업들은 모두 Job으로 표현됨
  - 매일 심야에 수행되는 **일일 매출 집계**
  - 매주 일요일마다 처리되는 **휴면 회원 정리**
  - 매월 1일에 실행되는 **정기 결제**

### Step

- Job을 구성하는 실행단위. Job은 하나 이상의 Step으로 구성됨
  - 예를 들어 **일일 매출 집계** Job은 다음과 같은 Step들로 이뤄질 수 있음
    - **1. 매출 집계 Step**
      - 전일 주문 데이터를 읽고 (read)
      - 결제 완료된 것만 필터링하여 (process)
      - 상품별/카테고리별로 집계하여 저장 (write)
    - **2. 알림 발송 step**
      - 집계 요약 정보를 생성하여 관리자에게 전달
    - **3. 캐시 갱신 step**
      - 집계된 데이터로 캐시 정보 업데이트

## Spring Batch의 핵심 컴포넌트

- Job과 Step
- JobLauncher
  - Job을 실행하고 실행에 필요한 파라미터를 전달하는 역할
  - 배치 작업 실행의 시작점
- JobRepository
  - 배치 처리의 모든 메타데이터를 저장하고 관리하는 핵심 저장소
  - Job과 Step의 실행 정보(시작/종료 시간, 상태, 결과 등)를 기록
  - 이렇게 저장된 정보들은 배치 작업의 모니터링이나 문제 발생 시 재실행에 활용됨
- ExecutionContext
  - Job과 Step 실행 중의 상태 정보를 key-value 형태로 담는 객체
  - Job과 Step 간의 데이터 공유나 Job 재시작 시 상태 복원에 사용
- 데이터 처리 컴포넌트 구현체
  - ItemReader 구현체 (다양한 데이터 소스로부터 데이터를 읽어올 수 있음)
    - JdbcCursorItemReader
    - JpaPagingItemReader
    - MongoCursorItemReader
  - ItemWriter 구현체
    - JdbcBatchItemWriter
    - JpaItemWriter
    - MongoItemWriter

## Step의 처리 모델

- 테스크릿 지향 처리 (Tasklet Oriented Processing)
  - 비교적 복잡하지 않은 단순한 작업을 실행할 떄 사용
- 청크 지향 처리 (Chunk Oriented Processing)

### 태스크릿 지향 처리

- 매일 새벽 불필요한 로그 파일 삭제
- 특정 디렉토리에서 오래된 파일을 아카이브
- 사용자에게 단순한 알림 메시지 또는 이메일 발송
- 외부 API 호출 후 결과를 단순히 저장하거나 로깅

#### RepeatStatus의 두 얼굴 : FINISHED vs CONTINUABLE

- `RepeatStatus.FINISHED` : "다 끝났다. 이제 Step을 종료해도 된다"
  - Step의 처리가 성공이든 실패든 상관없이 해당 Step이 완료되었음을 의미
  - 더 이상 반복할 필요 없이 다음 스텝으로 넘어가며, 배치 잡은 차근차근 진행
- `RepeatStatus.CONTIUABLE` : "작업 진행 중, 추가 실행이 필요하다"
  - Tasklet의 `execute()` 메서드가 추가로 더 실행되어야 함을 Spring Batch Step에 알리는 신호
  - Step의 종료는 보류되고, 필요한 만큼 `execute()` 메서드가 반복 호출

#### RepeatStatus가 필요한 이유 : 짧은 트랜잭션을 활용한 안전한 배치 처리

"반복 작업 이라면 while문으로 처리하면 되는거 아닌가?"

Spring Batch는 Tasklet의 execute() 호출마다 새로운 트랜잭션을 시작하고 execute()의 실행이 끝나 RepeatStatus가 반환되면 해당 트랜잭션을 커밋한다.

예를 들어, 오래된 주문 데이터를 정리하는 배치 작업을 생각해보자. 한 번에 만 건씩 데이터를 삭제하는데, 총 100만 건의 데이터를 처리해야한다고 하자

- `execute()` 내부에서 while문을 사용한다면 : 80만 건째 처리;중 예외가 발생했을 때, 이미 처리한 79만 건의 데이터도 모두 롤백되어 하나도 정리되지 않은 상태로 돌아감
- `RepeatStatus.CONTINUABLE`로 반복한다면 : 매 만 건 처리마다 트랜잭션이 커밋되므로, 예외가 발생하더라도 79만 건의 데이터는 이미 안전하게 정리된 상태로 남음

결국, RepeatStatus를 반환해 `execute()`를 반복 실행하도록 하는 이유는 거대한 하나의 트랜잭션 대신 **작은 트랜잭션들로 나누어 안전하게 처리**하기 위해서다

### 태스크릿 지향 처리 - 한눈에 정리하기

- **단순하고 명확한 작업을 수행**하는 데 사용되는 Step 유형
- 파일 삭제, 데이터 초기화, 알림 발송 등 비교적 단순한 작업에 적합
  - **단순 작업에 적합**
    - 태스크릿 지향 처리는 알림 발송, 파일 복사, 오래된 데이터 삭제 등 단순 작업을 처리하는 Step 유형이다.
  - **Tasklet 인터페이스 구현**
    - Tasklet 인터페이스를 구현해 필요한 로직을 작성한 뒤, 이를 `StepBuilder.tasklet()` 메서드에 전달해 Step을 구성한다.
  - **RepeatStatus로 실행 제어**
    - `Tasklet.execute()` 메서드는 `RepeatStatus`를 반환하며, 이를 통해 실행 반복 여부를 결정할 수 있다.
  - **트랜잭션 지원**
    - Spring Batch는 Tasklet.execute() 메서드 실행 전후로 트랜잭션을 시작하고 커밋하여, 데이터베이스의 일관성과 원자성을 보장한다.

### Step의 또 다른 얼굴 - 청크 지향 처리

- Spring Batch를 접하며 다룰 대부분의 배치 작업, 특히 **데이터를 다루는 작업은 읽기 -> 처리 -> 쓰기**라는 공통된 패턴을 보인다.
- 이 방식을 Spring Batch에서는 **청크 지향 처리**라고 부른다.

- **Chunk - 데이터를 작은 덩어리로 나누어 처리하는 방식**

  - **청크(Chunk)는 데이터를 일정 단위로 쪼갠 덩어리**를 말한다.
  - Spring Batch에서 데이터 기반 처리 방식을 청크 지향 처리라고 부르는 이유는,
  - **읽고, 처리하고, 쓰는 작업을 일정 크기로 나눈 데이터 덩어리(청크)를 대상으로 하기 떄문**이다.

- **왜 Spring Batch는 이런 방법을 택했을까?**

  - 100만 건의 데이터를 한 번에 메모리로 불러오고, 처리하고, 저장한다면?
  - 결과는
    - 메모리는 터지고,
    - DB는 과부하로 비명을 지른다
    - 전체 시스템이 폭발한다.
  - **1. 메모리를 지켜라 - 데이터 폭탄 방지**
    - 청크 지향 처리는 다르다
    - 100개씩 나눠서 불러온다
    - 개념적으로, 메모리엔 단 100개의 데이터만 존재한다
  - **2. 가벼운 트랜잭션 - 작은 실패**
    - 트랜잭션은 작업의 성공 또는 실패를 하나의 단위로 묶는 것
    - 100만 건을 하나의 트랜잭션으로 처리한다면?
    - 작업 중간에 오류가 발생하면, 100만 건이 전부 롤백된다
    - 만약 작업 중간에 에러가 발생하면?
      - 이전 청크는 이미 커밋 완료
      - 에러가 발생한 청크만 롤백되고 해당 청크부터 재시작하면 된다

- 그렇다면 이 패턴이 Spring Batch에서 어떻게 구체화될까?

  - Spring Batch는 이를 세 가지 컴포넌트로 구현한다
  - 바로 **ItemReader, ItemProcessor, ItemWriter**

- **청크 지향 처리의 3대장 - 읽고, 깎고, 쓴다**

  - ItemReader - 데이터를 끌어오는 수혈관
    - 데이터를 읽어오는 것은 청크 지향 처리의 생명줄이다. **ItemReader**는 데이터라는 피를 시스템으로 수혈한다.
    ```java
    public interface ItemReader<T> {
      T read() throws Exception,
        UnexpectedInputException,
        ParseException,
        NonTransientResourceException;
    }
    ```
    - 한 번에 하나씩, 차례차례:
    - `read()` 메서드의 반환 타입을 주목하라. \*\*read() 메서드는 아이템을 하나씩 반환한다.
    - 여기서 아이템이란 파일의 한 줄 또는 데이터베이스의 한 `행(row)`에 해당하는 데이터 하나를 의미한다.
    - 예를 들어, 총 100만 건의 레코드가 있다면, 각각의 레코드를 아이템이라고 부른다. ItemReader는 데이터 소스(DB, 파일 등)에서 하나씩 순차적으로 읽어온다. 읽을 데이터가 더 이상 없으면 `null`을 반환하며, 스텝은 종료된다.
    - **ItemReader가 `null`을 반환하는 것이 청크 지향 처리 Step의 종료 시점이라는 점을 반드시 기억하라** 이는 Spring Batch가 Step의 완료를 판단하는 핵심 조건이다.
    - 다양한 구현체 제공
      - Spring Batch는 파일, 데이터베이스, 메시지 큐 등 다양한 데이터 소스에 대한 표준 구현체를 제공한다.
      - 예를 들어, FlatFileItemReader는 CSV나 텍스트 파일에서 데이터를 읽어온다,
      - JdbcCursorItemReader는 관계형 데이터베이스로부터 데이터를 읽어 온다.
  - ItemProcessor - 아이템 깎기
    - ```java
      public interface ItemProcessor<I, O> {
        O process(I item) throws Exception;
      }
      ```
    - 데이터 가공
      - 입력 데이터 (`I`)를 원하는 형태 (`O`)로 변환한다.
      - 예를 들어, 읽어온 원본 데이터를 비즈니스 로직에 맞게 가공하거나, 출력 시스템이 요구하는 형식으로 변환하는 작업이다.
    - 필터링
      - process() 메서드가 **null을 반환**하면 해당 입력 데이터는 처리 흐름에서 제외된다.
      - 다시 말해 ItemWriter로 전달되지 않는다.
      - 유효하지 않은 데이터나 처리할 필요가 없는 데이터를 걸러낼 떄 사용한다.
    - 데이터 검증
      - 입력 데이터의 유효성을 검사한다.
      - 필터링과 달리 조건에 맞지 않는 데이터를 만나면 **예외를 발생**시킨다.
      - 예를 들어, 필수 필드 누락이나 잘못된 형식을 발견했을 떄 예외를 던져 배치 잡을 중단시킨다.
    - 필수 아님
      - ItemProcessor는 생략 가능하다. 다시 말해 스텝이 데이터를 읽고 바로 쓰도록 구성할 수 있다.
  - ItemWriter - 데이터를 처형하는 최종 집행자
    - ItemWriter는 ItemProcessor가 만든 결과물을 받아, 원하는 방식으로 최종 저장/출력한다.
    - 데이터를 DB에 INSERT, 파일에 WRITE, 메시지 큐에 PUSH
    - ```java
      public interface ItemWriter<T> {
        void write(Chunk<? extends T> chunk) throws Exception;
      }
      ```
    - 한 덩어리씩 쓴다
      - Chunk 단위로 묶어서 한 번에 데이터를 쓴다.
    - 다양한 구현체 제공
      - Spring Batch는 파일, 데이터베이스, 외부 시스템 전송 등에 사용할 수 있는 다양한 구현체를 제공한다.

- **Reader-Processor-Writer 패턴**
  - 완벽한 책임 분리
  - 재사용성 극대화
    - 컴포넌트들은 독립적으로 설계되어 있어 어디서든 재사용 가능하다.
    - 새로운 배치를 만들 때도 기존 컴포넌트들을 조합해서 빠르게 구성할 수 있다.
  - 높은 유연성
    - 요구사항이 변경되어도 해당 컴포넌트만 수정하면 된다.
  - 대용량 처리의 표준

### 청크 지향 처리의 흐름

- **1. 데이터 읽기 (ItemReader)**
  - read() 메서드가 호출될 떄 마다 데이터를 순차적으로 반환한다.
  - 청크 크기만큼 데이터를 읽어야 끝난다.
  - 다시 말해, **청크 크기가 10이면 `read()`가 10번 호출되어 하나의 청크가 생성된다.**
- **2. 데이터 깎기 (ItemProcessor)**
  - **청크의 각 아이템** 하나씩을 처리한다.
  - 다시 말해, **청크 크기가 10이면 `process()`가 10번 호출된다.**
- **3. 데이터 쓰기 (ItemWriter)**

  - **청크 단위로 데이터를 저장한다.**

- 자 이제 청크 크기가 10일 떄의 실제 처리 흐름을 살펴보자.

  - 읽기 : `read()`가 10번 호출돼서 10개의 데이터를 가져와 하나의 **청크(inputs)**를 생성한다.
  - 깎기 : 청크의 각 아이템을 대상으로 `process()`가 10번 호출된다.
  - 쓰기 : 처리된 **청크(outputs)**가 `write()`에 전달된다. `write()` 메서드는 청크를 한 번에 저장하거나 출력한다.
  - 이 과정이 모든 데이터를 전부 처리할 때까지 반복된다. 우리 강의에서는 이를 청크 단위 반복이라고 부른다.

- 끝은 어디인가? - '청크 단위 반복'의 종료 조건

  - 더 이상 읽을 데이터가 없을 때가 모든 데이터를 처리한 떄다.
  - ItemReader의 read() 메서드가 null을 반환할 떄다.

- 청크 처리와 트랜잭션
  - 데이터를 청크 단위로 처리한다는 건 트랜잭션도 청크 단위로 관리된다는 의미.
  - 즉, 각각의 청크 단위 반복마다 별도의 트랜잭션이 시작되고 커밋된다.
  - 대용량 데이터를 처리하는 도중 중간에 스텝이 실패하더라도, 이전 청크 반복에서 처리된 데이터는 이미 안전하게 커밋되어 있어 그만큼의 작업은 보존된다.
  - 실패한 청크의 데이터만 롤백되므로, 전체 데이터를 처음부터 다시 처리할 필요가 없다.
